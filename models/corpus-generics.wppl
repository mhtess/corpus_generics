// time ~/webppl-fork/webppl corpus-generics.wppl --require utils generic 1
var chain = last(process.argv) // load index as last command line index
// penultimate argument is the semantics
// generic = uncertain threshold
// some = fixed threshold at lowest threshold value
var targetUtterance = process.argv[process.argv.length - 2]

var responseDictionary = { "agree-key": 1, "disagree-key": 0 };

var prior_dataPath = "../data/pilots/pilot-ccepe-2/",
    listener_dataPath = "../data/pilots/pilot-listener-prevalence-2/",
    endorse_dataPath = "../mturk/endorsement-1/"
var priorFile = prior_dataPath + "pilot-ccepe-2-trials.csv",
 		listenerFile = listener_dataPath + "pilot-listener-prevalence-2-trials.csv",
    endorseFile = endorse_dataPath + "endorsement-1-trials.csv"
     // listenerFile = listener_dataPath + "pilot-listener-prevalence-1-trials.csv";

var d_prior = dataFrame(utils.readCSV(priorFile).data, ["response"]),
 		d_listener =  dataFrame(utils.readCSV(listenerFile).data, ["response"]),
    d_endorse =  dataFrame(utils.readCSV(endorseFile).data, ["prevalence","response"]);
    // d_listener =  dataFrame(utils.readCSV(listenerFile).data, ["freq"]);

var data = {
	prior: map(function(d){
		extend(d, {
			avoided_endval: avoidEnds(d.response),
  		roundedPrevalence: utils.closest(midBins, d.response),
      property: d.verb_phrase.replace("&quotechar", "").replace(",", "")
		})
	}, filter(function(d){return d.trial_type == "prevalence_prior" }, d_prior)),
	listener: map(function(d){
		extend(d, {
      // slicing midBins removes the possibility that people can give the lowest slider response
			roundedPrevalence: utils.closest(midBins.slice(1), d.response),
      property: d.verb_phrase.replace("&quotechar", "").replace(",", ""),
      generic: d.noun_phrase + " " + d.verb_phrase.replace("&quotechar", "").replace(",", "")
		})
	}, filter(function(d){return d.workerid != "1" }, d_listener)),
  endorse: map(function(d){
		extend(d, {
  		roundedPrevalence: utils.closest(midBins, d.prevalence / 100),
      property: d.verb_phrase.replace("&quotechar", "").replace(",", "")
		})
	}, d_endorse)
};


// console.log(data.endorse)

var utterancePrior = Infer({model: function(){return uniformDraw([targetUtterance,"silence"])}});

var items = levels(data.prior, "property")

var addNoise = function(dist, propNoise){
  Infer({model: function(){
    return flip(propNoise) ? uniformDraw([0, 1]) : sample(dist)
  }})
}

var meaning = function(utt,state, theta) {
  return utt === "generic" ? state > theta :
         utt === "generic is false" ? state<=theta :
         utt === "silence" ? true :
         true
}

var nullParams = {a:1, b:100}, nullDistribution = Beta(nullParams);

var model = function(){

  var speakerOptimality = uniformDrift({a:0, b:10, width: 1});

	foreach(items, function(item){

		var sentenceData = {
			listener: _.filter(data.listener, {property: item}),
			prior: _.filter(data.prior, {property: item}),
      endorsement: _.filter(data.endorse, {property: item})
		}

    // console.log(sentenceData.prior)

		// prior parameters
		var theta = 1//uniformDrift({a: 0, b: 1, width:0.2})

		var betaParams = {
			g: uniformDrift({a: 0, b: 1, width: 0.2}),
			d: uniformDrift({a: 0, b: 100, width: 20})
		}

    // var otherBetaParams = {
		// 	g: uniformDrift({a: 0, b: 1, width: 0.2}),
		// 	d: uniformDrift({a: 0, b: 100, width: 20})
		// }

		var priorParams1 = betaShape(betaParams);
    // var priorParams2 = betaShape(otherBetaParams);
    var priorParams2 = nullParams;
    // console.log(item + " " + _.map(sentenceData.prior, "roundedPrevalence"))
    // display("before prevalence prior factor")
		// observe structured prior data
		mapData({data: sentenceData.prior}, function(d){
			// display(d.roundedPrevalence)
			// display(Delta({v: 0}).score(d.roundedPrevalence))
			// display(Beta(priorParams).score(d.roundedPrevalence))
			var scr = util.logsumexp([
				 Math.log(theta) + Beta(priorParams1).score(d.avoided_endval),
         Math.log(1-theta) + Beta(priorParams2).score(d.avoided_endval)
				 // Math.log(1-theta) + nullDistribution.score(d.roundedPrevalence)
				//  Math.log(1-theta) + Delta({v: 0}).score(d.roundedPrevalence)
			 ])
       // console.log(scr)
			 factor(scr)
		})

    // var priorScr = sum(map(function(d){return util.logsumexp([
    //    Math.log(theta) + Beta(priorParams).score(d.avoided_endval),
    //    Math.log(1-theta) + nullDistribution.score(d.avoided_endval)
    //   //  Math.log(1-theta) + Delta({v: 0}).score(d.roundedPrevalence)
    //  ])}, sentenceData.prior))
    //
    // query.add(["prior", "score", item, "NA"], priorScr )

    // display("after prevalence prior factor")

		query.add(["prior","mixture", item, "NA"], theta)
		query.add(["prior","stableFreq1", item, "mean"], betaParams.g)
		query.add(["prior","stableFreq1", item, "sampleSize"], betaParams.d)
    // query.add(["prior","stableFreq2", item, "mean"], otherBetaParams.g)
		// query.add(["prior","stableFreq2", item, "sampleSize"], otherBetaParams.d)

		var statePrior = Infer({model: function(){
			sample(flip(theta) ?
      DiscretizedBeta(priorParams1) :
      DiscretizedBeta(priorParams2)
    )
		}});

		/// RSA model
		var listener0 = cache(function(utterance) {
		  Infer({model: function(){
		    var state = sample(statePrior)
				var theta = sample(thetaPrior);
        // condition (state > theta)
        // factor( Math.log(state) )
		    var m = meaning(utterance, state, theta)
		    condition(m)
		    return state
		 }})}, 10000)

    var l0predictions = listener0("generic")
		var responseData = _.map(sentenceData.listener, "roundedPrevalence")

		mapData({data:responseData}, function(d){
			var scr = l0predictions.score(d)
			scr == -Infinity ? display(item + " "  + d) : null
			observe(l0predictions, d)
		})

    query.add(["listener", targetUtterance, item, sentenceData.listener[0].generic], expectation(l0predictions) )


    var speaker1 = cache(function(prevalence) {
     Infer({model: function(){
       var utterance = sample(utterancePrior);
       var L0 = listener0(utterance);
       factor(speakerOptimality * L0.score(prevalence))
       return utterance === targetUtterance ? 1 : 0
     }})}, 10000)


    var prevalenceLevels = levels(sentenceData.endorsement, "roundedPrevalence");

    foreach(prevalenceLevels, function(prevalence){
      var s1predictions = speaker1(prevalence)
      var prevResponseData = _.map(_.filter(sentenceData.endorsement,
        {roundedPrevalence: prevalence}),"response")

        mapData({data:prevResponseData}, function(d){
     			var scr = s1predictions.score(d)
     			scr == -Infinity ? display(item + " " + prevalence + " " + d) : null
     			observe(s1predictions, d)
     		})
      query.add(["endorsement", prevalence, item, sentenceData.listener[0].generic], expectation(s1predictions) )

    })


	})

	query.add(["param","speakerOptimality","s1","NA"], speakerOptimality)
  // query.add(["param","noise","s1","na"], noise)
  // query.add(["param","fixedThreshold","s1","na"], fixedThreshold)

	return query
}

// 1500 iterations / min (including burn-in)
var totalIterations = 10000, lag = 20;
var mhiter = totalIterations/lag, burn = totalIterations / 2;

var outfile = 'results-corpgen-singleBetaPrior-L0-S1partialData-smntcs_'+targetUtterance+"-"+ totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+chain+'.csv'

var posterior = Infer({
  model: model,
	method: "incrementalMH",
  samples: mhiter, burn: burn, lag: lag,
  verbose: T,
  verboseLag: mhiter / 10,
	stream: {
		path: "results/" + outfile,
		header: ["type", "param", "property", "category", "val"]
	}
})

"written to " + outfile;
